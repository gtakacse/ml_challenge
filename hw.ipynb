{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:  (1367, 4)\n",
      "Test set:  (586, 4)\n",
      "Recall: 0.6917562724014337\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "\n",
    "data = []\n",
    "\n",
    "# Betöltés\n",
    "# kódolási séma specifikálása\n",
    "with open('data.csv','r', encoding='utf-8-sig') as f:\n",
    "    csvreader = csv.DictReader(f)\n",
    "    for item in csvreader:\n",
    "        data.append([ item['DATE'], item['AUTHOR'], item['CONTENT'], item['CLASS'] ])\n",
    "\n",
    "# A 'data' tömb elemei: ['dátum string', 'szerző', 'komment', 'osztály cimke ('0': nem spam, '1': spam)']\n",
    "        \n",
    "# Train/test szétválasztás\n",
    "split = 0.7\n",
    "data = np.asarray(data)\n",
    "perm = np.random.permutation(len(data))\n",
    "\n",
    "train = data[perm][0:int(len(data)*split)]\n",
    "test = data[perm][int(len(data)*split):]\n",
    "\n",
    "print('Train set: ', np.shape(train))\n",
    "print('Test set: ', np.shape(test))\n",
    "\n",
    "# Buta osztályozó\n",
    "def dumb_classify(data):\n",
    "    threshold = 0.3\n",
    "    if random.random() > threshold:\n",
    "        return '1'\n",
    "    else:\n",
    "        return '0'\n",
    "\n",
    "\n",
    "# Használd a 'train' adatokat az osztályozó módszer kidolgozására, a 'test' adatokat kiértékelésére!\n",
    "# Lehetőleg használj gépi tanulást!\n",
    "# Dokumentáld az érdekesnek tartott kísérleteket is!\n",
    "\n",
    "# Példa kiértékelés 'recall' számításával. \n",
    "# Kérdés: Milyen egyéb metrikát használnál kiértékelésre és miért? \n",
    "sum_positive = 0\n",
    "found_positive = 0\n",
    "\n",
    "for datapoint in test:\n",
    "    if datapoint[-1] == '1':\n",
    "        sum_positive += 1\n",
    "        if dumb_classify(datapoint) == '1':\n",
    "            found_positive += 1\n",
    "    \n",
    "print('Recall:', found_positive / sum_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importálandó modulok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re\n",
    "import emoji\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "tokenizer = TweetTokenizer()\n",
    "stemmer = EnglishStemmer()\n",
    "stp_wrds = set(stopwords.words('english') + [''])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Előfeldolgozás"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train, columns=[\"date\", \"author\", \"text\", \"spam\"])\n",
    "test_df = pd.DataFrame(test, columns=[\"date\", \"author\", \"text\", \"spam\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['want',\n",
       " 'to',\n",
       " 'win',\n",
       " 'borderlands',\n",
       " 'the',\n",
       " 'pre',\n",
       " '-',\n",
       " 'sequel',\n",
       " '?',\n",
       " 'check',\n",
       " 'my',\n",
       " 'channel',\n",
       " ':)',\n",
       " '\\ufeff']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = train_df['text'].iloc[2]\n",
    "tokenizer.tokenize(x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_word(word):\n",
    "    # szamok helyettesitese\n",
    "    if re.match(r'\\d+', word) is not None:\n",
    "        return '$'\n",
    "    # emoji es smiley helyettesitese\n",
    "    elif re.match(r'(?::|;|=)(?:-)?(?:\\)|D|P|\\()', word) is not None or word in emoji.UNICODE_EMOJI:\n",
    "        return 'EMOJI'\n",
    "    # url linkek helyettesitese\n",
    "    elif re.match(r'((http|https):\\/\\/)?([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])?', word) is not None:\n",
    "        return 'URL'\n",
    "    else:\n",
    "        # irasjelek eltavolitasa kiveve felkialtojel\n",
    "        out = re.sub(r'[^A-Za-z0-9!]', '', word)\n",
    "        # ha egy betu egymas utan tobb, mint 3szor szerepel egy szoban, akkor valoszinuleg emfatikus a hasznalata\n",
    "        # pl. woooowwwwwwww\n",
    "        # ezeket egyetlen beture cserelem\n",
    "        out = re.sub(r'(.)\\1+', r'\\1', out)\n",
    "        # stemmelés\n",
    "        return stemmer.stem(out)\n",
    "    \n",
    "def preprocessor(comment_text):\n",
    "    # \\ufeff karakterek eltűntetése\n",
    "    clean_text = comment_text.replace('\\ufeff', ' ')\n",
    "    # html kodok eltuntetese\n",
    "    clean_text = re.sub(r'<br\\s*\\/*>', ' ', clean_text)\n",
    "    # kisbetűsítés és tokenizálás\n",
    "    token_list = tokenizer.tokenize(clean_text.lower())\n",
    "    # elofeldolgozes, tagekre csereles, stemmeles\n",
    "    p_token_list = [process_word(word) for word in token_list]\n",
    "    # stop word eltavolitas\n",
    "    return [item for item in p_token_list if item not in stp_wrds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Egy példa a preprocesszor működéséről\n",
      "want to win borderlands the pre-sequel? check my channel :)﻿\n",
      "['want', 'win', 'borderland', 'pre', 'sequel', 'check', 'chanel', 'EMOJI']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['haha', 'danc', 'tight', 'know', 'EMOJI', '!', '!', '!']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Egy példa a preprocesszor működéséről')\n",
    "print(x)\n",
    "print(preprocessor(x))\n",
    "preprocessor('HAHAA THIS DANCE IS TIGHTTTT<br /><br />I know :) !!!\"^*&^?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vektorizálás"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vektorizalas eloszor count vektorizatorral\n",
    "\"\"\"vectorizer = CountVectorizer(lowercase=False,\n",
    "                             ngram_range=(1, 1), \n",
    "                             analyzer=preprocessor, \n",
    "                             max_df=0.95, \n",
    "                             min_df=5, \n",
    "                             max_features=None)\n",
    "\"\"\"\n",
    "vectorizer = TfidfVectorizer(lowercase=False,\n",
    "                            ngram_range=(1,1),\n",
    "                            analyzer=preprocessor,\n",
    "                            max_df=0.95,\n",
    "                            min_df=5)\n",
    "X_train = vectorizer.fit_transform(train_df['text'])\n",
    "y_train = train_df['spam'].map(lambda x: int(x))\n",
    "X_test = vectorizer.transform(test_df['text'])\n",
    "y_test = test_df['spam'].map(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering - extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#spams = train_df[train_df['spam'] == '1']\n",
    "#print(spams['text'].head(25))\n",
    "#train_df['date'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_char_ratios(comment):\n",
    "    if len(comment) < 1:\n",
    "        return (0, 0, 0)\n",
    "    # betűk aránya\n",
    "    lett_ratio = len(re.findall(r'[a-zA-Z]', comment))/len(comment)\n",
    "    # nagybetűk aránya\n",
    "    upper_ratio = len(re.findall(r'[A-Z]', comment))/len(comment)\n",
    "    # irasjelek aránya\n",
    "    punc_ratio = len(re.findall(r'[^a-zA-Z0-9\\s]', comment))/len(comment)\n",
    "    return (lett_ratio, upper_ratio, punc_ratio)\n",
    "    \n",
    "def deter_posting_time(date):\n",
    "    # posztolás ideje: 0 = NaN, 1 = hajnal, 2 = delelott, 3 = delutan, 4 = este \n",
    "    if len(date) < 1:\n",
    "        return 0\n",
    "    hour = int(re.search(r'T(\\d{2}):', date).groups()[0])\n",
    "    if hour < 6:\n",
    "        return 1\n",
    "    elif hour < 12:\n",
    "        return 2\n",
    "    elif hour < 18:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1367, 393) (586, 393)\n"
     ]
    }
   ],
   "source": [
    "# extra feature-ok hozzaadasa a tanitoadatokhoz\n",
    "n = X_train.shape[1]\n",
    "feature_names = ['lett_ratio', 'upper_ratio', 'punc_ratio', 'posting_time']\n",
    "extra_train = train_df['text'].map(lambda x: calc_char_ratios(x))\n",
    "extra_train = extra_train.apply(pd.Series)\n",
    "extra_train[3] = train_df['date'].map(lambda x: deter_posting_time(x))\n",
    "X_train = hstack((X_train, extra_train))\n",
    "\n",
    "#update vecotorizer vocabulary\n",
    "for i in range(4):\n",
    "    vectorizer.vocabulary_[feature_names[i]] = n + i\n",
    "\n",
    "# extra feature-ok hozzaadasa a tesztadatokhoz\n",
    "extra_test = test_df['text'].map(lambda x: calc_char_ratios(x))\n",
    "extra_test = extra_test.apply(pd.Series)\n",
    "extra_test[3] = test_df['date'].map(lambda x: deter_posting_time(x))\n",
    "X_test = hstack((X_test, extra_test))\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Multinomial Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A spam detektálás egy klasszifikációs feladat, amelyet az irodolom szerint a Naív Bayes osztályozók kifejezetten jól képesek kezelni. A Naív Bayes-féle osztályozók továbbá kellően robosztusak, gyorsak és kevés tanulóadattal is jól működnek, így baseline modellként érdemes ezzel kezdeni.\n",
    "\n",
    "Mivel a nyelvi adatok feldolgozásához gyakorisági alapú vektorizálót használtam, így a naív Bayes osztályozók multinomiális változatát választottam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall a tanulóadatokon\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.96      0.94       643\n",
      "          1       0.96      0.93      0.95       724\n",
      "\n",
      "avg / total       0.94      0.94      0.94      1367\n",
      "\n",
      "Recall a tesztadatokon\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.93      0.94       307\n",
      "          1       0.93      0.94      0.93       279\n",
      "\n",
      "avg / total       0.93      0.93      0.93       586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multi_nb = MultinomialNB()\n",
    "multi_nb.fit(X_train, y_train)\n",
    "print('Recall a tanulóadatokon')\n",
    "print(classification_report(y_train, multi_nb.predict(X_train)))\n",
    "print('Recall a tesztadatokon')\n",
    "print(classification_report(y_test, multi_nb.predict(X_test)))\n",
    "# performance logs\n",
    "# CountVectorizer max_df 1.0, min_df 1, MN Naive Bayes (alpha 1), test set: 0 recall 0.84, 1 recall 0.95\n",
    "# CountVectorizer max_df 1.0, min_df 1, MN Naive Bayes (alpha 1) with extra features, test set: 0 recall 0.89, 1 recall 0.95\n",
    "# CountVectorizer max_df 0.95, min_df 5, MN Naive Bayes (alpha 1) with extra features, test set: 0 recall 0.96, 1 recall 0.88\n",
    "# TfidfVectorizer max_df 1.0, min_df 1, MN Naive Bayes (alpha 1) with extra features, test set: 0 recall 0.95, 1 recall 0.91\n",
    "# TfidfVectorizer max_df 0.95, min_df 5, MN Naive Bayes (alpha 1) with extra features, test set: 0 recall 0.93, 1 recall 0.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: {'alpha': 2}\n",
      "Recall a tanulóadatokon\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.92      0.96      0.94       643\n",
      "          1       0.96      0.93      0.94       724\n",
      "\n",
      "avg / total       0.94      0.94      0.94      1367\n",
      "\n",
      "Recall a tesztadatokon\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.94      0.94      0.94       307\n",
      "          1       0.94      0.94      0.94       279\n",
      "\n",
      "avg / total       0.94      0.94      0.94       586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# alpha állítása\n",
    "mn_nb = MultinomialNB()\n",
    "params = {'alpha': [0.01, 0.1, 1, 2, 5, 10]}\n",
    "grid_CV = GridSearchCV(mn_nb, params, cv=10, refit=True)\n",
    "grid_CV.fit(X_train, y_train)\n",
    "print('Best alpha: {}'.format(grid_CV.best_params_))\n",
    "print('Recall a tanulóadatokon')\n",
    "print(classification_report(y_train, grid_CV.predict(X_train)))\n",
    "print('Recall a tesztadatokon')\n",
    "print(classification_report(y_test, grid_CV.predict(X_test)))\n",
    "# best model with TfidfVectorizer max_df 0.95, min_df 5, with extra featerus\n",
    "# MN_Bayes alpha 2, 0 recall on test dataset 0.94; 1 recall on test dataset 0.94"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Support Vector Machine algoritmusok ugyancsak jól szoktak szerepelni a szövegkategorizációs feladatokban, így a következő részben megnézem, hogy milyen pontossággal képesek a kommentek közül kiválogatni a spamokat. A Naív Bayes-féle modellek eddigi legjobbjai 94%-os recall körül teljesítettek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.01, 0.1, 1, 10], 'degree': [2], 'class_weight': ['balanced'], 'kernel': ['linear', 'poly', 'rbf', 'sigmoid']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring=None, verbose=0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_model = SVC()\n",
    "svc_params = {'C': [0.01, 0.1, 1, 10],\n",
    "             'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "             'degree': [2],\n",
    "             'class_weight': ['balanced']}\n",
    "svc_grid = GridSearchCV(svc_model, svc_params)\n",
    "svc_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'C': 10, 'degree': 2, 'class_weight': 'balanced', 'kernel': 'linear'}\n",
      "Recall a tanulóadatokon\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.99      0.99       643\n",
      "          1       0.99      0.98      0.99       724\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1367\n",
      "\n",
      "Recall a tesztadatokon\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.95      0.95       307\n",
      "          1       0.95      0.95      0.95       279\n",
      "\n",
      "avg / total       0.95      0.95      0.95       586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Best params: {}'.format(svc_grid.best_params_))\n",
    "print('Recall a tanulóadatokon')\n",
    "print(classification_report(y_train, svc_grid.predict(X_train)))\n",
    "print('Recall a tesztadatokon')\n",
    "print(classification_report(y_test, svc_grid.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Az SVM klasszifikátor 1%-kal teljesít jobban, mint a Naív Bayes modell. Átlagosan a recall 95% a tesztadatokon. Mivel a legjobb eredményt a lineáris kernel érte el, így érdemes lehel logaritmikus regresszióval is ellenőrizni az eredményeket, hiszen a scikit-learn Logistic Regression moduljában több "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[250,  47],\n",
       "       [ 14, 275]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, multi_nb.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# todo\n",
    "# SVM, random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lehetséges változók"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Url link; kulcsszabak *check out, visit, subscribe, follow me, money*\n",
    "2. Nagybetűk, írásjelek pl. felkiáltójelek aránya\n",
    "3. Felhasználó (csak 21 komment van ugyanazon felhasználótól a spamok között) ``np.unique(spams[:, 1])``\n",
    "4. Posztolás ideje"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modellek építése"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lehetséges problémak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Osztályok közötti aránytalanság"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spams = train[np.where(train == '1')[0]]\n",
    "len(train)/len(spams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Majdnem 2-szer annyi a negatív példa, mint pozitív."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# egyedi felhasznalonevek szama\n",
    "len(spams) - len(np.unique(spams[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
